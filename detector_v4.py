# -*- coding: utf-8 -*-
"""CNN+LSTM+NoTimestamp_Detector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CD-iMqjNd2m8oHaZ76bqs0H6MMAtpFFU
"""

from __future__ import print_function


import numpy as np # Biblioteca de funciones matematicas de alto nivel
np.random.seed(1337)  # for reproducibility
import tensorflow as tf
import keras
import pandas

import time #Calcular tiempo medio clasificación
import os #Biblioteca para ejecutar comandos en el terminal
from ipaddress import ip_address    #Comparar distintas IPs
from ipaddress import ip_network    #Comparar distintas IPs

import elasticsearch                                #Módulo de elasticsearch
from elasticsearch import Elasticsearch, helpers    #Añadidos para facilitar volcado de datos
import uuid                                         #Generar id universal aleatorio 

from datetime import datetime                       #Para convertir la fecha
import pytz

from keras.preprocessing import sequence
from keras.models import Sequential # necesario para poder generar la red neuronal
from keras.layers import Dense, Dropout, Activation, Lambda # Tipos de capa, hacen lo siguiente:
from keras.layers import Embedding
from keras.layers import Convolution1D,MaxPooling1D, Flatten, LSTM
from keras.callbacks import CSVLogger # para guardar los datos en un excel
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger

from keras.datasets import imdb # un dataset incluido en keras
from keras import backend as K # importas el backend (Tensorflow, Theano, etc)
import pandas as pd # pandas es una libreria extension de numpy usada para manipulacion y analisis de datos, para manipular tablas numericas y series temporales
from keras.utils.np_utils import to_categorical # sirve para convertir vectores de enteros a una matriz de clases binaria
import h5py # para almacenar un monton de datos numericos y dar facilidades de manipulacion para datos de Numpy
from sklearn.preprocessing import Normalizer # Para normalizar los datos
from sklearn.model_selection import train_test_split #para hacer la separacion entre datos de test y train
from sklearn.preprocessing import OneHotEncoder #para convertir los datos de entrada


##Incio función red neuronal

def red_neuronal():
    #Cargamos las etiquetas que son todos los ataques posibles que detecta nuestra red
    Labels =['Benign',
             'DoS attacks-GoldenEye',
             'DoS attacks-Hulk',
             'DoS attacks-SlowHTTPTest',
             'DoS attacks-Slowloris',
             'FTP-BruteForce',
             'SSH-Bruteforce']
    #creamos la red neuronal
    model = Sequential()
    model.add(Convolution1D(64, 3, activation="relu",input_shape=(77, 1), padding = 'same'))
    #######
    #   Añadimos la primera capa de Convolution1D, los diferentes parametros indican lo siguiente:
    #       64 --> numero de filtros
    #       3 --> tamaño del filtro (3,1)
    #       border_mode = "same" --> este parametro sirve para que el output sea del mismo tamaño que elinput
    #       activation = "relu" --> Tipo de funcion de activacion de neuronas que vamos a usar
    #       input_shape = (77, 1) --> tamaño de la entrada, hay 77 features (quitamos timestamp y dstport con respecto al original)
    #######
    model.add(Convolution1D(64, 3, activation="relu", padding = 'same'))
    model.add(MaxPooling1D(pool_size=(2))) # capa donde se lleva a cabo el pooling, se queda con el maximo de cada 2
    model.add(Convolution1D(128, 3,  activation="relu", padding = 'same'))
    model.add(Convolution1D(128, 3,  activation="relu", padding = 'same'))
    model.add(MaxPooling1D(pool_size=(2)))
    model.add(Flatten()) #Si eliminamos red LSTM
    model.add(Dropout(0.1)) #
    model.add(Dense(len(Labels), activation="softmax")) # capa fully conected para decision final, usamos softmax porque con ella los valores finales tienen mas relacion con los valores
    # anteriores y no solo con 1

    # define optimizer and objective, compile cnn
    model.compile(loss="categorical_crossentropy", optimizer="adam",metrics=['categorical_accuracy']) # se compila la red neuronal con los siguientes parametros:
    ######
    #   PARAMETROS:
    #       loss = "categorical_crossentropy" --> esta relacionado con la funcion softmax, se usa para dar una probabilidad sobre unas clases
    #       optimizer = "adam" --> Stochastic gradient descent
    #       metrics = 'accuracy' --> compara los resultados finales con los reales y de ahi saca las estadisticas
    ######


    #Cargamos los resultados obtenidos en la etapa de train
    # model.load_weights("E:/TFG/ResultadosRedes/Modelos/Modelos_sin/saved_model_sin_5.0.hdf5")
    
    #Configuración para que funcione servicio de firewall de Windows:
    
    # Configure el servicio firewall de Windows para que se inicie automáticamente
    os.system('sc config mpssvc start=auto')
    # Inicie el servicio firewall de Windows
    os.system('net stop mpssvc && net start mpssvc')
    # Habilite los perfiles firewall de Windows
    os.system('netsh advfirewall set allprofiles state on')
    
    # Conectamos con Elasticsearch 
    elastic = Elasticsearch("http://localhost:9200")
    
    # Diccionario de mapeo que contiene también los ajustes
    # Esquema de mapeo para el índice que vamos a crear:
    mapping = {
        "settings": {
            "number_of_shards": 2,
            "number_of_replicas": 1
        },
        "mappings": {
            "properties": {
                'Protocolo':{
                    'type':'long'
                },
                'Ip Src': {
                    'type':'ip'
                },
                'Ip Dst': {
                    'type':'ip'
                },
                'Port Src': {
                    'type':'long'
                },  
                'Port Dst': {
                    'type':'long'
                },             
                'Etiqueta': {
                    'type':'keyword'
                },
                '@timestamp': {
                    'type':'date'
                }
            }
        }
    }
           
    #Borramos el index si ya existía para evitar fallos al ejecutar nuestro código
    if elastic.indices.exists(index='prueba_def'):
        elastic.indices.delete(index='prueba_def')
    
    #Creamos el índice con la estructura indicada en la variable mapping
    response = elastic.indices.create(
        index="prueba_def",
        body=mapping,
        ignore=400 # ignore 400 already exists code
    )   
    
    return model 
    ##Fin función red neuronal
    
    
    # Inicio función que obtiene términos correctos normalización
def normalizador():

    dataset = pd.read_csv('E:/TFG/Datasets_Nuevos/Concatenados/Train.csv', encoding = "ISO-8859-1") # lectura de datos
    
    # Replacing infinite and nan para evitar errores
    dataset.replace([np.inf, -np.inf], -1, inplace=True) 
    dataset.replace([np.nan, -np.nan], -1, inplace=True)
    #Eliminamos los datos mal introducidos
    dataset = dataset.drop(dataset[dataset['Dst Port']=='Dst Port'].index)
    #Eliminamos las columnas innecesarias
    dataset = dataset.drop(['Flow ID', 'Src IP', 'Src Port', 'Dst IP','Unnamed: 0','Dst Port'], axis=1)
    dataset = dataset.drop(['Timestamp'], axis=1)
    
    #Convertimos los datos a float64 para poder trabajar con ellos (menos el campo de etiquetas)
    n=0
    for column in dataset:
        column
        if column != 'Label':
            dataset[column] = dataset[column].astype(float)
            
    X_train=dataset.iloc[:, 0:77] 
    
    
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler().fit(X_train)    
   

    return scaler

#Inicio tratamiento de datos
def clasificador(dataset,model,filas_n,condicion,scaler):

    #Configuración para que no salte warning falso positivo python pandas
    pd.options.mode.chained_assignment = None  # default='warn'
    
    #Nos quedamos con la quintupla para poder generar reglas
    quintupla = dataset.loc[:,['Src IP', 'Src Port', 'Dst IP','Dst Port','Protocol']]
    
    dataset['Timestamp'] = dataset['Timestamp'].apply(lambda x: x.replace("a.","A"))
    dataset['Timestamp'] = dataset['Timestamp'].apply(lambda x: x.replace("p.","P"))
    dataset['Timestamp'] = dataset['Timestamp'].apply(lambda x: x.replace("m.","M"))
    dataset['Timestamp'] = dataset['Timestamp'].apply(lambda x: x.replace("\xa0",""))
    
    format = '%d/%m/%Y %I:%M:%S %p'
    
    
    for i in dataset.index:
        dataset['Timestamp'][i]=datetime.strptime(dataset['Timestamp'][i],format)

    fecha = dataset['Timestamp']
    UTC_OFFSET_TIMEDELTA = datetime.utcnow() - datetime.now()
    fecha_utc= fecha+UTC_OFFSET_TIMEDELTA
    
    # Replacing infinite and nan para evitar errores
    dataset.replace([np.inf, -np.inf], -1, inplace=True) 
    dataset.replace([np.nan, -np.nan], -1, inplace=True)
    
    #Eliminamos los datos mal introducidos
    dataset = dataset.drop(dataset[dataset['Dst Port']=='Dst Port'].index)
    #Eliminamos las columnas innecesarias
    dataset = dataset.drop(['Flow ID', 'Src IP', 'Src Port', 'Dst IP','Dst Port'], axis=1)
    #Quitamos direcciones IP dest y src para no introducir sesgos, lo mismo con los puertos
    #Quitar el puerto de destino es una modificación con respecto al trabajo original
    #Eliminamos la columna de timestamp para no tener sesgo
    dataset = dataset.drop(['Timestamp'], axis=1)

    #Convertimos los datos a float64 para poder trabajar con ellos (menos el campo de etiquetas)
    n=0
    for column in dataset:
        column
        if column != 'Label':
            dataset[column] = dataset[column].astype(float)
            
    Y = dataset["Label"]
            
    #Establecemos nuestra entrada que vamos a testear
    #Todo menos las etiquetas
    X=dataset.iloc[:, 0:77]

    #Cargamos las etiquetas que son todos los ataques posibles que detecta nuestra red
    Labels =['Benign',
             'DoS attacks-GoldenEye',
             'DoS attacks-Hulk',
             'DoS attacks-SlowHTTPTest',
             'DoS attacks-Slowloris',
             'FTP-BruteForce',
             'SSH-Bruteforce']
     
    testT = scaler.transform(X)# Asi se representan los datos

    # reshape input to be [samples, time steps, features]
    X_test = np.reshape(testT, (testT.shape[0],testT.shape[1],1))
    
    #Cargamos los resultados obtenidos en la etapa de train
    model.load_weights("E:/TFG/ResultadosRedes/Modelos/Modelos_sin/saved_model_sin_train.hdf5")

    #predecimos la naturaleza de los paquetes de test
    #y_pred = (model.predict(X_test) > 0.5).astype("int32")     #Para clase binaria
    y_pred=np.argmax(model.predict(X_test), axis=-1)            #Para multiclase
    y_pred_etiq=y_pred;
    
    from sklearn.preprocessing import LabelEncoder #Para pasar a numeros
    lb_make = LabelEncoder()
    lb_make.fit_transform(Labels)
    y_pred_etiq=lb_make.inverse_transform(y_pred)

    if condicion==0:                                                                                    #Si es la primera vez que se ejecuta creamos el archivo 
        np.savetxt('E:/TFG/ResultadosRedes/Resultados/No_DstPort/predicted.txt', y_pred_etiq, fmt='%s') #Guardamos la predicción
    else:                                                                                               #Si se dan ejecuciones sucesivas
        with open("E:/TFG/ResultadosRedes/Resultados/No_DstPort/predicted.txt","a") as f:               #Abrimos el archivo
            np.savetxt(f, y_pred_etiq, fmt="%s")                                                        #Escribimos en el las nuevas líneas
    
    command='netsh advfirewall firewall add rule name = "BLACKLIST" dir=in action=block remoteip='  #Comando que bloquea IP en windows
    localIP='192.168.64.131'     #Por si quisiesemos comparar con una sola IP directamente     
    localIPs='192.168.64.131/32' #El paramétro 32 es modificable para casos generales
    lista_ips=[]                 #Lista que usaremos para bloquear la IP a la segunda 
    
    # Conectamos con Elasticsearch 
    elastic = Elasticsearch("http://localhost:9200")
    
    actions=[]
    counter=0    
        
    for index, value in np.ndenumerate(y_pred):         #Recorremos el array donde tenemos los resultados de la clasificación
                  
        if condicion==1:
            indice=index[0]+filas_n
            counter=counter+filas_n 
        else:
            indice=index[0]
       
        if y_pred[index]!=0:                         #Si el valor es distinto de 0 (benigno), entonces es un ataque y entramos
          print(indice,y_pred_etiq[index])           #Mostramos por pantalla el índice y el tipo de ataque detectado
          print(quintupla['Src IP'][indice])         #Imprimimos la dirección IP del posible atacante
           
          if ip_address(quintupla['Src IP'][indice]) not in ip_network(localIPs): #Si la dirección atacante no está dentro de la red local entonces se bloquea
           
            if quintupla['Src IP'][indice] in lista_ips:  #Si la dirección del atacante se ha visto previamente (se ha guardado en lista_ips) bloqueamos :
                attackIP=quintupla['Src IP'][indice]          #Guardamos en la dirección IP del atacante en una variable auxiliar
                command=command+attackIP                        #Construimos el comando para bloquear la IP del atacante
                # os.system(command)                            #Ejecutamos el comando para bloquear la dirección IP del atacante
                print(command)                                  #Imprimimos como sería comando para verlo ya que no lo ejecutamos por seguridad
                command='netsh advfirewall firewall add rule name = "BLACKLIST" dir=in action=block remoteip='  #Finalmente limpiamos el comando, para poder seguir usandolo con otras IPs
                
            else:                                           #Si no está en la lista_ips es que no la habíamos visto antes y entonces no se bloquea 
                lista_ips.append(quintupla['Src IP'][indice]) #Pero se añade a la lista para bloquearla si la volvemos a ver

        
        source ={
                      "Protocolo":quintupla['Protocol'][indice],
                      "Ip Src": quintupla['Src IP'][indice], 
                      "Ip Dst": quintupla['Dst IP'][indice], 
                      "Port Src": quintupla['Src Port'][indice], 
                      "Port Dst": quintupla['Dst Port'][indice], 
                      "Etiqueta": y_pred_etiq[index], 
                      "@timestamp": fecha_utc[indice] 
                } 
        action = {
            "_index": 'prueba_def',
            '_op_type': 'index',
            "_type": "_doc",
            "_id": counter,
            "_source": source
                }
        actions.append(action)
        counter = counter+1
        if len(actions) >= 1000:
              helpers.bulk(elastic, actions)
              del actions[0:len(actions)]
       
    if len(actions) > 0:
       helpers.bulk(elastic, actions)    
  
