{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c42bc0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np # Biblioteca de funciones matematicas de alto nivel\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas\n",
    "import time #Calcular tiempo medio clasificación\n",
    "import os #Biblioteca para ejecutar comandos en el terminal\n",
    "from ipaddress import ip_address    #Comparar distintas IPs\n",
    "from ipaddress import ip_network    #Comparar distintas IPs\n",
    "from datetime import datetime                       #Para convertir la fecha\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential # necesario para poder generar la red neuronal\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda # Tipos de capa, hacen lo siguiente:\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D,MaxPooling1D, Flatten, LSTM\n",
    "from keras.callbacks import CSVLogger # para guardar los datos en un excel\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.datasets import imdb # un dataset incluido en keras\n",
    "from keras import backend as K # importas el backend (Tensorflow, Theano, etc)\n",
    "import pandas as pd # pandas es una libreria extension de numpy usada para manipulacion y analisis de datos, para manipular tablas numericas y series temporales\n",
    "from keras.utils.np_utils import to_categorical # sirve para convertir vectores de enteros a una matriz de clases binaria\n",
    "import h5py # para almacenar un monton de datos numericos y dar facilidades de manipulacion para datos de Numpy\n",
    "from sklearn.preprocessing import Normalizer # Para normalizar los datos\n",
    "from sklearn.model_selection import train_test_split #para hacer la separacion entre datos de test y train\n",
    "from sklearn.preprocessing import OneHotEncoder #para convertir los datos de entrada\n",
    "from sklearn.preprocessing import LabelEncoder #Para pasar a numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7669732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tensorflow/python/client/session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fddeddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Incio función red neuronal\n",
    "def red_neuronal():\n",
    "    #Cargamos las etiquetas que son todos los ataques posibles que detecta nuestra red\n",
    "    Labels =['Benign',\n",
    "             'DoS attacks-GoldenEye',\n",
    "             'DoS attacks-Hulk',\n",
    "             'DoS attacks-SlowHTTPTest',\n",
    "             'DoS attacks-Slowloris',\n",
    "             'FTP-BruteForce',\n",
    "             'SSH-Bruteforce']\n",
    "             \n",
    "    #creamos la red neuronal\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(64, 3, activation=\"relu\",input_shape=(77, 1), padding = 'same'))\n",
    "    model.add(Convolution1D(64, 3, activation=\"relu\", padding = 'same'))\n",
    "    model.add(MaxPooling1D(pool_size=(2))) # capa donde se lleva a cabo el pooling, se queda con el maximo de cada 2\n",
    "    model.add(Convolution1D(128, 3,  activation=\"relu\", padding = 'same'))\n",
    "    model.add(Convolution1D(128, 3,  activation=\"relu\", padding = 'same'))\n",
    "    model.add(MaxPooling1D(pool_size=(2)))\n",
    "    model.add(Flatten()) #Si eliminamos red LSTM\n",
    "    model.add(Dropout(0.1)) #\n",
    "    model.add(Dense(len(Labels), activation=\"softmax\")) # capa fully conected para decision final, usamos softmax porque con ella los valores finales tienen mas relacion con los valores\n",
    "    # anteriores y no solo con 1\n",
    "\n",
    "    # define optimizer and objective, compile cnn\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=['categorical_accuracy']) # se compila la red neuronal con los siguientes parametros:\n",
    "\n",
    "    #Cargamos los resultados obtenidos en la etapa de train\n",
    "    model.load_weights(\"ResultadosRedes/saved_model_sin.hdf5\")\n",
    "    \n",
    "    return model \n",
    "    ##Fin función red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f13ffe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicio tratamiento de datos\n",
    "def clasificador(dataset,model):\n",
    "\n",
    "    #Configuración para que no salte warning falso positivo python pandas\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    #Nos quedamos con la quintupla para poder generar reglas\n",
    "    quintupla = dataset.loc[:,['Src IP', 'Src Port', 'Dst IP','Dst Port','Protocol']]\n",
    "    \n",
    "    #Eliminamos los datos mal introducidos\n",
    "    dataset = dataset.drop(dataset[dataset['Dst Port']=='Dst Port'].index)\n",
    "    #Eliminamos las columnas innecesarias\n",
    "    dataset = dataset.drop(['Flow ID', 'Src IP', 'Src Port', 'Dst IP','Dst Port'], axis=1)\n",
    "    #Quitamos direcciones IP dest y src para no introducir sesgos, lo mismo con los puertos\n",
    "    #Quitar el puerto de destino es una modificación con respecto al trabajo original\n",
    "    #Eliminamos la columna de timestamp para no tener sesgo\n",
    "    dataset = dataset.drop(['Timestamp'], axis=1)\n",
    "\n",
    "    #Convertimos los datos a float64 para poder trabajar con ellos (menos el campo de etiquetas)\n",
    "    n=0\n",
    "    for column in dataset:\n",
    "        column\n",
    "        if column != 'Label':\n",
    "            dataset[column] = dataset[column].astype(float)\n",
    "    Y = dataset[\"Label\"]\n",
    "            \n",
    "    # Replacing infinite and nan para evitar errores\n",
    "    dataset.replace([np.inf, -np.inf], -1, inplace=True) \n",
    "    dataset.replace([np.nan, -np.nan], -1, inplace=True)\n",
    "\n",
    "    #Establecemos nuestra entrada que vamos a testear\n",
    "    #Todo menos las etiquetas\n",
    "    X=dataset.iloc[:, 0:77]\n",
    "\n",
    "    #Cargamos las etiquetas que son todos los ataques posibles que detecta nuestra red\n",
    "    Labels =['Benign',\n",
    "             'DoS attacks-GoldenEye',\n",
    "             'DoS attacks-Hulk',\n",
    "             'DoS attacks-SlowHTTPTest',\n",
    "             'DoS attacks-Slowloris',\n",
    "             'FTP-BruteForce',\n",
    "             'SSH-Bruteforce']\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler().fit(X) # Normalizamos los datos\n",
    "    testT = scaler.transform(X)# Asi se representan los datos\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    X_test = np.reshape(testT, (testT.shape[0],testT.shape[1],1))\n",
    "    \n",
    "    #Cargamos los resultados obtenidos en la etapa de train\n",
    "    model.load_weights(\"ResultadosRedes/saved_model_sin.hdf5\")\n",
    "\n",
    "    #predecimos la naturaleza de los paquetes de test\n",
    "    #y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")     #Para clase binaria\n",
    "    y_pred=np.argmax(model.predict(X_test), axis=-1)            #Para multiclase\n",
    "    y_pred_etiq=y_pred;\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder #Para pasar a numeros\n",
    "    lb_make = LabelEncoder()\n",
    "    lb_make.fit_transform(Labels)\n",
    "    y_pred_etiq=lb_make.inverse_transform(y_pred)\n",
    "    \n",
    "    #Configuración para que funciones servicio de firewall de Windows\n",
    "    # os.system('sc config mpssvc start=auto')\n",
    "    # os.system('net stop mpssvc && net start mpssvc')\n",
    "    # os.system('netsh advfirewall set allprofiles state on')\n",
    "    \n",
    "    command='netsh advfirewall firewall add rule name = \"BLACKLIST\" dir=in action=block remoteip='  #Comando que bloquea IP en windows\n",
    "    localIP='192.168.64.131'     #Por si quisiesemos comparar con una sola IP directamente     \n",
    "    localIPs='192.168.64.131/32' #El paramétro 32 es modificable para casos generales\n",
    "    lista_ips=[]                 #Lista que usaremos para bloquear la IP a la segunda \n",
    "\n",
    "\n",
    "    for index, value in np.ndenumerate(y_pred):         #Recorremos el array donde tenemos los resultados de la clasificación\n",
    "        \n",
    "    \n",
    "        if y_pred[index]!=0:                           #Si el valor es distinto de 0 (benigno), entonces es un ataque y entramos\n",
    "          # print(index[0],y_pred_etiq[index])           #Mostramos por pantalla el índice y el tipo de ataque detectado\n",
    "          # print(quintupla['Src IP'][index[0]])         #Imprimimos la dirección IP del posible atacante\n",
    "           \n",
    "          if ip_address(quintupla['Src IP'][index[0]]) not in ip_network(localIPs): #Si la dirección atacante no está dentro de la red local entonces se bloquea\n",
    "           \n",
    "            if quintupla['Src IP'][index[0]] in lista_ips:  #Si la dirección del atacante se ha visto previamente (se ha guardado en lista_ips) bloqueamos :\n",
    "                attackIP=quintupla['Src IP'][index[0]]          #Guardamos en la dirección IP del atacante en una variable auxiliar\n",
    "                command=command+attackIP                        #Construimos el comando para bloquear la IP del atacante\n",
    "                # os.system(command)                            #Ejecutamos el comando para bloquear la dirección IP del atacante\n",
    "                #print(command)                                  #Imprimimos como sería comando para verlo ya que no lo ejecutamos por seguridad\n",
    "                command='netsh advfirewall firewall add rule name = \"BLACKLIST\" dir=in action=block remoteip='  #Finalmente limpiamos el comando, para poder seguir usandolo con otras IPs\n",
    "                \n",
    "            else:                                           #Si no está en la lista_ips es que no la habíamos visto antes y entonces no se bloquea \n",
    "                lista_ips.append(quintupla['Src IP'][index[0]]) #Pero se añade a la lista para bloquearla si la volvemos a ver\n",
    "  \n",
    "\n",
    "        \n",
    "#Hacer las dos medidas de ancho de banda sin elastic y con elastic pasandole un dataset grande para ver comportamiento real "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2733748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63e5304a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263101\n",
      "Se han clasificado 263100 flujos en 0.00011957751349866367\n",
      "El tiempo total ha sido  31.460843801498413 segundos\n",
      "El ancho de banda en bytes/s es  568365.9380791418\n",
      "El ancho de banda en bits/s es  4546927.504633134\n"
     ]
    }
   ],
   "source": [
    "#Declaración de variables auxiliares\n",
    "filePath='Datasets/BW_prueba.csv'\n",
    "# filePath='E:/TFG/CICFlowMeter/CICFlowMeter-master/build/install/CICFlowMeter/bin/data/daily/2022-02-14_Flow.csv'\n",
    "dataset_size=1\n",
    "t_total=0\n",
    "\n",
    "#Construcción red neuronal y la guardamos en model \n",
    "model=red_neuronal()\n",
    "\n",
    "#Abrimos el archivo y lo guardamos en una variable llamada f\n",
    "with open(filePath) as f:                                          #Bucle que se ejecuta constantemente\n",
    "    dataset=pd.read_csv(filePath, encoding = \"ISO-8859-1\")  #pd.read_csv lee el archivo csv entero  \n",
    "    if dataset.shape[0]>dataset_size:                       #Si el número de filas del archivo leido (nº flujos) es mayor que el nº filas anterior entonces clasifico sino no \n",
    "        t_inicio=time.time()                                #Hallamos el tiempo antes de la clasificación\n",
    "        clasificador(dataset,model)                       #Línea que se encarga de llamar a la red neuronal para clasificar nuevos flujos\n",
    "        t_fin=time.time()                                   #Hallamos el tiempo después de la clasificación\n",
    "        t_total=(t_fin-t_inicio)+t_total;                     #Hallamos el tiempo total\n",
    "        t_medio=(t_fin-t_inicio)/(dataset.shape[0]-dataset_size) #Obtenemos el tiempo medio, restamos el número de filas total del dataset menos el anterior porque son los nuevos flujos que clasifica, inicialmente el anterior vale 1 (fila de características)\n",
    "\n",
    "        n_bytes=dataset['Subflow Fwd Byts']+dataset['Subflow Bwd Byts']     #Sumamos la columna de enviados y recibidos para tener el columna con el total de bytes\n",
    "        n_bytes_tot=n_bytes.sum()                                           #Obtenemos la suma de todas las filas para hallar el total de bytes \n",
    "\n",
    "        print(\"Se han clasificado\",(dataset.shape[0]-dataset_size),\"flujos en\",t_medio) #Obtenemos el número de flujos clasificados por tiempo\n",
    "        print(\"El tiempo total ha sido \",t_total, \"segundos\")                           #Vemos el tiempo total\n",
    "        print(\"El ancho de banda en bytes/s es \",n_bytes_tot/t_total)                   #Obtenemos el ancho de banda dividiendo el número de bytes total entre el tiempo total\n",
    "        print(\"El ancho de banda en bits/s es \",(n_bytes_tot*8)/t_total)                #Obtenemos el ancho de banda en bits/s\n",
    "        dataset_size=dataset.shape[0]                                                   #Actualizamos el valor del número de filas \n",
    "\n",
    "#El bucle se ejecuta contantemente, se lee el archivo csv que genera el CICFLOWMETER, si el número de filas del dataset leído es mayor que 1\n",
    "#entonces ya tiene un flujo (la primera fila indica las características). Y se lee constantemente el dataset viendo el núemero de filas. Si \n",
    "#el número de filas es mayor que el almacenado en dataset_size (lectura anterior) es que han llegado flujos nuevos y entonces clasificamos.\n",
    "\n",
    "#Para calcular el tiempo medio se saca el tiempo antes y después de la clasificación y se hace una resta. Para calcularlo cada nueva llegada \n",
    "#de flujos, lo que hacemos es restar el número total de flujos menos el que había en la lectura anterior, obteniendo los nuevos flujos que \n",
    "#se van a clasificar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
